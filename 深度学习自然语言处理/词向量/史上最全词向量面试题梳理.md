**微信公众号：NLP从入门到放弃**

1. 有没有使用自己的数据训练过Word2vec，详细说一下过程。包括但是不限于：语料如何获取，清理以及语料的大小，超参数的选择及其原因，词表以及维度大小，训练时长等等细节点。
2. Word2vec模型是如何获得词向量的？聊一聊你对词嵌入的理解？如何理解分布式假设？
3. 如何评估训练出来的词向量的好坏 
4. Word2vec模型如何做到增量训练
5. 大致聊一下 word2vec这个模型的细节，包括但不限于：两种模型以及两种优化方法（大致聊一下就可以，下面会详细问）
6. 解释一下 hierarchical softmax 的流程(CBOW and Skip-gram)
7. 基于6，可以展开问一下模型如何获取输入层，有没有隐层，输出层是什么情况。
8. 基于6，可以展开问输出层为何选择霍夫曼树，它有什么优点，为何不选择其他的二叉树
9. 基于6，可以问该模型的复杂度是多少，目标函数分别是什么，如何做到更新梯度（尤其是如何更新输入向量的梯度）
10. 基于6，可以展开问一下 hierarchical softmax 这个模型 有什么缺点
11. 聊一下负采样模型优点（为什么使用负采样技术）
12. 如何对输入进行负采样（负采样的具体实施细节是什么）
13. 负采样模型对应的目标函数分别是什么（CBOW and Skip-gram）
14. CBOW和skip-gram相较而言，彼此相对适合哪些场景
15. 有没有使用Word2vec计算过句子的相似度，效果如何，有什么细节可以分享出来
16. 详细聊一下Glove细节，它是如何进行训练的？有什么优点？什么场景下适合使用？与Word2vec相比，有什么区别（比如损失函数）？
17. 详细聊一下Fasttext细节，每一层都代表了什么？它与Wod2vec的区别在哪里？什么情况下适合使用Fasttext这个模型？
18. ELMO的原理是什么？以及它的两个阶段分别如何应用？（第一阶段如何预训练，第二阶段如何在下游任务使用）
19. ELMO的损失函数是什么？它是一个双向语言模型吗？为什么？
20. ELMO的优缺点分别是什么？为什么可以做到一词多义的效果？





本面试题词向量资源参考：

word2vec、glove、cove、fastext以及elmo对于知识表达有什么优劣？ - 霍华德的回答 - 知乎
https://www.zhihu.com/question/292482891/answer/492247284

面试题：Word2Vec中为什么使用负采样？ - 七月在线 七仔的文章 - 知乎
https://zhuanlan.zhihu.com/p/66088781

关于word2vec，我有话要说 - 张云的文章 - 知乎
https://zhuanlan.zhihu.com/p/29364112

word2vec（二）：面试！考点！都在这里 - 我的土歪客的文章 - 知乎
https://zhuanlan.zhihu.com/p/133025678

nlp中的词向量对比：word2vec/glove/fastText/elmo/GPT/bert - JayLou娄杰的文章 - 知乎
https://zhuanlan.zhihu.com/p/56382372

史上最全词向量讲解（LSA/word2vec/Glove/FastText/ELMo/BERT） - 韦伟的文章 - 知乎
https://zhuanlan.zhihu.com/p/75391062

word2vec详解（CBOW，skip-gram，负采样，分层Softmax） - 孙孙的文章 - 知乎
https://zhuanlan.zhihu.com/p/53425736

Word2Vec详解-公式推导以及代码 - link-web的文章 - 知乎
https://zhuanlan.zhihu.com/p/86445394

关于ELMo，面试官们都怎么问：https://cloud.tencent.com/developer/article/1594557