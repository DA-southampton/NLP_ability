文本图像特征表示和融合-多模态讲解系列文章

接上一个文章聊一下在多模态中文本和图像是如何做到模型特征表示和融合的。

最直觉的一个思路是，我们分别对文本和图像进行特征建模，然后对特征使用某种方式进行融合。所以就分成两个模块：特征表示模块和特征融合模块。

特征表示模块

对于文本特征的表示，我们这么去想：对一个视频，我们能够想到的文本一般是：标题+简介+字幕。

那么我么如何对文本进行建模呢？之前写一个关于句向量综述的文章，里面有介绍一部分内容。

1. 词袋模型（基于统计和基于词向量），这种建模问题在于忽略了词序信息，可以使用n-gram进行缓解。

2. 基于任务（CNN/RNN），存在的问题是迁移性较差。如果是分类网络训练出来的CNN表达的句子向量迁移到情感分类效果不会很好。然后我们分开来说，CNN存在一个问题就是对长距离处理的不是很好。
因为它的本质是重视的n-gram内的语序信息。RNN存在的问题是训练速度慢，这没什么可说的，不能并行是硬伤。

还有其他建模方式就不多说了。我们来看爱奇艺的处理方式，一句话简单描述是“采用的是 BOW 和 CNN+Attention 方式完成文本表示的建模”

Bow使用一些人工特征加n-gram缓解自带的问题。CNN使用两个优化，提取信息使用一定步长的pooling，然后基于这个带有文本信息的表达做self-attention。


对于短视频来说的图像表示是什么？是封面。封面一般是从短视频精选出来的一帧，一定程度可以对文本信息进行补充。

对图像进行特征的抽取一般是有三种方式：

1. 直接抽取特征
实现方式：把 ImageNet 预训练的模型作为特征抽取器，将模型的某一层或者某几层特征作为类型标签模型特征提取源。缺点是效果比较差。

一般对应到NLP，大家可以想一下我们直接用Bert抽取出来的词向量做文本分类，效果也比较差。

2. finetune+抽取特征
把 ImageNet 预训练的模型以类型标签为目标进行 FineTune，然后将模型的某一层或者某几层特征作为类型标签模型特征提取源（因训练目标一致，一般选择最后一层即可达到较好的效果）。

大家仔细琢磨一下这个过程。如果我先在要做一个文本分类的任务，想使用LR做一个baseline。那么我的输入可以是这样的，使用bert对我要使用分类数据（注意是和LR一样的训练数据）进行FineTune
，然后使用这个模型做特征的抽取。

FineTune的任务和我LR要做的是一样的，那么bert抽取的特具有充足的意义表达，能够很好的迁移过来。

基于此，大家可以想一下，如果我使用bert做了文本情感分析的FineTune，然后抽取的特征做文本分类，效果会好吗？想一下。

这还有一个问题，从bert抽取出来的特征，我们需不需要随着模型进行修改？？？


3. 把 ImageNet 预训练的模型嵌入到类型标签的模型当中，让图像的表示和其他特征的表示同时进行训练。

其实这种方式缺点很明显，耗时太大了，有种尾大不掉的感觉。

爱奇艺选择的第二种，模型选择是 Xception进行特征的抽取。

接下来我们聊一下文本和图像的特征融合怎么做，也就是文本图像的特征怎么联系在一起？

我给大家两个思路，第一个就是直接concat，然后接你的各种网络。第二种是两个特征模块做 attention，这个更常见一点。

爱奇艺还给出其他两种方式CentralNet 和LMF，我没了解过，就不多说了。

对于attention，一般来说做双向的attention更合适一点。也就是说做一个文本到图像的attention，然后做一个图像到文本的attention，两者再concat，效果会更好。
