TextCNN中的卷积核在文本进行处理的时候，是在文本长度上进行卷积。卷积核的大小
不同，带来的直接后果是这个卷积核每次滑动的时候处理的单词长度不同。
卷积核大小为2的时候，一次处理2-gram。卷积核大小为3-gram，一次处理三个大小的单词。
所以卷积核在对文本进行卷积的操作，更像是对在提取文本在n-gram上的特征。卷积核权重的更新
只是为了能够更好的提取n-gram上的特征。卷积核权重的更新
大小为2的卷积核提取的是2-gram特征，大小为3的卷积核提取的是3-gram特征，以此类推。
取不同卷积核大小进行卷积操作的原因，我的理解是可以提取这个句子多个维度不同的信息，使得特征更加的丰富。

还有一点需要去注意的是，以2-gram为例，每次都是提取两个单词文本，但是如果文本很长，最后两个字和最开始的维度的单词
联系就很小，唯一的联系就是卷积核的权重是共享的。
举个例子：
今天天气不错，适合出去旅游

在这句话中，如果卷积核大小为2，我们这里不考虑中文分词，那么今天 天天 两个词组中间出了有卷积核权重的联系还有天这个单词的共有性。
但是今天和旅游两个单词联系性在CNN中并没有体现出来。
这也就是为什么CNN不适合处理长文本的原因。


卷积之后，接了一个最大池化。论文中给出的原因是因为输入句子长度不一定，经过卷积之后长度不一定，
如果直接操作的话，后面的全连接层权重形状不固定，不利于训练。
其实感觉这一点站不住脚，处理文本的时候，一般会固定长度，阶段长度，不存在卷积之后大小不一定的原因。

但是如果我们在处理文本的时候，没有截断长度，而是排序然后按照batch中长读补长，是存在上述问题的，所以需要最大池化。

上面这个原因感觉是最重要的，其实还有一个原因，论文中是说想要获取一个卷积核提取出来特征中的最重要的特征。我的可理解是
这个原因不太好，因为我直接用所有特征肯定比选取其中一个最重要的效果是好的。


论文中把一个卷积核抽取特征，然后接一个最大池化的操作，形象的比喻为一个卷积核抽取一个特征。


有一个人把特点总结的很到位，叫做CNN的卷积核实现了捕捉局部相关性
