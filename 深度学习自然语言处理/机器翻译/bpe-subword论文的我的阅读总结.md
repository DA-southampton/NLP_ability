bpe论文的我的阅读感受

Neural Machine Translation of Rare Words with Subword Units

提出这个算法的直觉是这样的，作者发现翻译一个单词，有时候不需要这个单词的全部信息，可能只需要一部分信息就可以知道大致信息。还有一种可能是翻译这个单词与，可能通过组成
这个单词的多个小单元信息来翻译就可以了。

这个方法是为了解决稀少单词（也就是频率在人为规定下的单词）和未登录词没有办法有效翻译的问题。

这里作者在摘要中提到了一个back-off 字典，就是说在之前翻译模型在处理未登录词汇的时候，处理办法是使用一个词典，把source-target中未登陆词汇一一对应起来，我们在翻译过程中
如果出现了未登录词汇，直接使用字典中的对应关系进行替换就可以了。但是这样存在一个问题，就是说，最低频率是我们人为规定的，有些时候在调参的时候，这个频率是一个超参，，那么
我们在准备词典的时候，就是一个动态的长度，这样很不方便，但是如果我们准备所有单词的back-off就得不偿失。还有一个问题是我们不确定source-target对应的关系是一一对应的，可能对应不上，可能对应
是多种，在不同句子环境中，我们需要选择不同的单词翻译，这也是存在的一个问题。

基于word-level的模型还存在一个问题，就是不能产生没有看见过的单词，也就是说在翻译端，没有出现在词汇表中的在翻译模型测试的时候是不会出现的。这其实是一个很重要的问题，就是我不能确定
我的训练语料包含所有情况下的翻译。

作者在摘要中说明，自己使用了字符级的n-gram和bpe方法，在WMT 15 英文德文翻译中提升1.1，在英文俄罗斯中提升1.3。



翻译是一个开放词汇表的问题，我们在翻译模型中，一般把翻译模型词汇表限制在30000–50000（基于词）。

作者通过实验发现，使用subeword模型，比使用大量词汇表的模型和使用back-off模型效果很好更简单。



我在博客中看到了总结这个论文不错的博客，总结在下面
通过BPE解决OOV问题----Neural machine Translation of Rare Words with Subword Units
https://blog.csdn.net/weixin_38937984/article/details/101723700

