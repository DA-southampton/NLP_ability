在做中文NER的时候，我们的常规操作是以字为单词。这是因为如果以词为单位，

很容易造成切分错误，导致误差的积累。

我举个简单的例子，比如我现在有一句话，【你去北京老哥】

但是以字为单词，有一个问题就是会忽视词的信息。

所以，大家很自然就想仍然以字为单词做NER，但是把词的信息补充进来。

这个时候，一个很朴素的想法就是，我输入的时候过一遍分词，然后把词向量和字向量拼接或者相加或者做别的操作来融合起来。

这个方法一般来说能够提升准确度，但是不会太多。

后来还有一种思想就是使用 lattice structure，这种确实做到了词汇信息的增强，但是存在并行化困难以及推理速度慢的缺点，换句话说，方法是好方法，但是落地困难。

这个论文做了一个什么事情呢？把栅栏式结构通过相对位置编码展平。

我们知道transformer为了保持位置信息，对于每个token，是使用了位置编码的。在这里，为了这个晶格结构设计了一个巧妙的位置编码，来把复杂结构展开展平：

如图所示：

![ Flat-Lattice Transformer](https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-08-051016.jpg)

看这个图需要注意的是，【重】这个字对应到英文代表的是character-字符，【重庆】这个词组对应到英文代表的是word-单词，这一点，大家在读论文的时候需要注意。

为每一个token（包含char和word）分配两个位置索引：头位置和尾位置；

在原来的晶格结构中，比如【店】只能和【人和药店】以及【药店】产生关系，但是在TRM中，由于self-attention的存在，【店】是可以和序列中的每个token都发生关系，不仅仅是和self-matched的词汇。这算是一个意外之喜。

self-matched的词汇，就是包含当前char的



谈一下为什么这么转化：

一般来说，我们有语料，和词典，通过词典，我们可以得到一个晶格

# 为什么要把晶格结构压平

头部的索引就是第一个单词的位置，尾部就是最后一个单词所在的位置，如果是一个char，头尾就是相同的。

通过这个巧妙的设置，我们是可以把展平的东西再重建到晶格模式的，所以认为是可行的。

# 相对位置编码

通过头尾索引，我们可以把晶格结构压平。

现在还面临一个问题，就是对于【人和药店】头尾索引是【3】【6】，但是这并不包含位置信息。

对于NER来说，位置信息是很重要的。

对于普通的TRM，使用绝对位置编码保持位置信息，但是有研究表示，这种位置信息在self-attention中使用向量内积的时候，会减弱。

具体的大家可以看我这个文章：[原版Transformer的位置编码究竟有没有包含相对位置信息](https://mp.weixin.qq.com/s?__biz=MzIyNTY1MDUwNQ==&mid=2247483760&idx=1&sn=c2803e63bdd42e4d1f1f880ce9eda8cc&chksm=e87d3356df0aba40c77356418647856ec135c731fd60122378ed702e1e959c820250c2293e1f&token=588814416&lang=zh_CN#rd)；

所以，我们现在就要考虑使用相对位置信息来表达位置，同时还要把我们头尾索引融合进来。

对于句子中的两个spans（包含char和words）$x_{i},x_{j}$，它们可能有三种关系：相交，包含，和分离。

比如上面那个例子，【药店】和【人和药店】就是包含的关系；【重庆】和【人和药店】就是分离的关系。

我们使用一个向量来描述两个spans之间的关系。

先说两个spans之间存在的距离关系可以用如下公式去表达：

![实体距离关系公式](https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-08-051014.jpg)

上角标的$(hh)$代表的就是两个spans之间的头部索引差值，其他上角标类似的意思。

具体的实际是什么样子，大家可以看上面的图c；

然后我们使用如下的公式去生成相对位置编码：

![相对位置编码](https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-08-051015.jpg)

接下来的问题就是利用这个相对位置编码融入到TRM之中。

![attention_new矩阵](https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-08-051012.jpg)

**简单来说，就是利用相对位置编码，生成了一个包含相对位置编码信息的新的attention矩阵，不再使用原始的attention矩阵**

看到这里，其实有注意到一个很有意思的点就是FLAT使用的是一层encoder。

# 实验

实验比较感兴趣的是

一个是和其他词汇增强的网络结果相比，效果如何。

还有一个就是使用transformer之后，TRM长距离依赖的优点和每个token之间都可以交互的优点有没有在提升效果上发挥作用

还有一个其实很自然的会想到能不能使用将FLAT和BERT融合起来。也就是如何将动态的字向量和FLAT这种词向量结合起来。

先看第一二点

![results in different models](https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-08-051013.jpg)

再看第三点

![BERT+FLAT](https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-08-51014.jpg)

有意思的是，使用了FLAT之后，在Resume和Weibo效果有提升，但是不明显，作者认为可能是因为数据集有点小。在大数据集Ontonotes和MSRA上，效果提升比较明显。

推理速度的话，和Lattice LSTM相比，BSZ为16的情况下，基本是8倍左右。

# 总结

梳理一下怎么把词汇信息加入进去的：

1. 首先我们知道NER融合词汇信息能提升最终效果，但是一般的Lattice结构落地困难
2. 然后受TRM位置信息的启发，将Lattice结构展开
3. 然后由于普通TRM绝对位置信息在self-attention中会被削弱，所以想要使用相对位置信息。
4. 从头尾索引，我们可以知道tokens之间有三种关系：相交，包含，隔离；从这三种关系，我们可以得到两个tokens的四种距离公式，并且把这个四种距离公式融入到了相对位置信息。
5. 得到最终的相对位置信息，将相对位置信息融合进入attention矩阵，参与Encoder计算